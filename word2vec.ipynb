{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/corpus.txt', 'r') as file:\n",
    "    content = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(training_data):\n",
    "    all_words = \" \".join(training_data).lower()\n",
    "    all_words = all_words.replace('\\ufeff', '')\n",
    "    all_words = all_words.replace('.', '')\n",
    "    all_words = all_words.split(' ')\n",
    "    vocab = list(set(all_words))\n",
    "    vocab.sort()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(word, vocab, vocab_size):\n",
    "    one_hot = [0] * vocab_size\n",
    "    pos = vocab.index(word)\n",
    "    one_hot[pos] = 1\n",
    "    one_hot = np.array(one_hot)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_word_map(vocab, vocab_size):\n",
    "    vec2word = {str(one_hot(word, vocab, vocab_size)): word \\\n",
    "                      for word in vocab}\n",
    "    return  vec2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = content\n",
    "\n",
    "\n",
    "vocab = create_vocabulary(training_data)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "for word in vocab:\n",
    "    print(f'{word}:{\" \"*(6-len(word))} {one_hot(word, vocab, vocab_size)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_training_data(training_data, vocab, vocab_size, window_size):\n",
    "    encoded_training_data = []\n",
    "    for sentence in training_data:\n",
    "        tokens = re.sub(r'[^\\w\\s]', '', sentence).lower().split(' ')\n",
    "        for word_pos, word in enumerate(tokens):\n",
    "            center_word = one_hot(word, vocab, vocab_size)\n",
    "            outside_words = []\n",
    "            for outside_pos in range(word_pos-window_size, \n",
    "                                     word_pos+window_size+1):\n",
    "                if (outside_pos >= 0) and (outside_pos < len(tokens)) \\\n",
    "                and (outside_pos != word_pos):\n",
    "                    outside_words.append(one_hot(tokens[outside_pos],\n",
    "                                                 vocab,\n",
    "                                                 vocab_size))\n",
    "            encoded_training_data.append([center_word, outside_words])\n",
    "    return encoded_training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = encode_training_data(\n",
    "    training_data = training_data, vocab = vocab,\n",
    "    vocab_size = vocab_size, window_size = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    probs = np.exp(x) / np.sum(np.exp(x))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(outside_words, y_pred):\n",
    "    combined_outside_words = np.sum(outside_words, axis = 0)\n",
    "    components = np.multiply(combined_outside_words, y_pred)\n",
    "    non_zero_idx = np.where(components != 0)[0]\n",
    "    non_zero_components = components[non_zero_idx]\n",
    "    log_components = np.log(non_zero_components)\n",
    "    loss = - np.sum(log_components)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 3\n",
    "\n",
    "w_center = np.random.rand(vocab_size, embedding_dim)\n",
    "w_outside = np.random.rand(embedding_dim, vocab_size)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "losses = []\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000 ):\n",
    "    loss = 0\n",
    "\n",
    "    for x, outside_word in training_data:\n",
    "        h = np.dot(x, w_center)\n",
    "        u = np.dot(h, w_outside)\n",
    "        y_pred = softmax(u)\n",
    "        loss += calculate_loss(outside_word, y_pred)\n",
    "        e = np.sum([y_pred - ow for ow in outside_word], axis = 0)\n",
    "        grad_w_outside = np.outer(h, e)\n",
    "        grad_w_center = np.outer(x, np.dot(w_outside, e))\n",
    "        w_outside = w_outside - (lr * grad_w_outside)\n",
    "        w_center = w_center - (lr * grad_w_center)\n",
    "    print(f'epoch: {epoch} loss: {loss}')\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(projection = '3d')\n",
    "\n",
    "for i, (x, y, z) in enumerate(w_center):\n",
    "    ax.scatter(x, y, z)\n",
    "    ax.text(x + 0.01, y + 0.01, z,  vocab[i], size = 13)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
